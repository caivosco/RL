{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref. https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivan/Environments/RL/ReinforcementLearningTutorialDQNPyTorch\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN_Pytorch_Tutorial.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.classic_control.cartpole.CartPoleEnv at 0x7f7f003f0c50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.envs.classic_control.cartpole.CartPoleEnv"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up matplotlib\n",
    "is_python = 'inline' in matplotlib.get_backend()\n",
    "if is_python:\n",
    "    from IPython import display\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We’ll be using experience replay memory for training our DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                       ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe main idea behind Q-learning is that if we had a function Q∗: State×Action → ℝ\\nthat could tell us what our return would be, if we were to take an action in a given state,\\n\\nthen we could easily construct a policy that maximizes our rewards:\\n\\nπ∗(s)=argmax wrt:a  for Q∗(s,a)\\n\\nSo we need to build a Q function using NN\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember that a neural networks are universal function approximators, we can simply \n",
    "# create one and train it to resemble any function\n",
    "\n",
    "'''\n",
    "The main idea behind Q-learning is that if we had a function Q∗: State×Action → ℝ\n",
    "that could tell us what our return would be, if we were to take an action in a given state,\n",
    "\n",
    "then we could easily construct a policy that maximizes our rewards:\n",
    "\n",
    "π∗(s)=argmax wrt:a  for Q∗(s,a)\n",
    "\n",
    "So we need to build a Q function using NN\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nFor our training update rule, we’ll use a fact that every Q function for some policy obeys the Bellman equation:\\n\\n  Qπ(s,a)=r+γQπ(s′,π(s′))\\n\\nThe difference between the two sides of the equality is known as the temporal difference error, δ\\n\\n  δ=Q(s,a)−(r+γmaxaQ(s′,a))\\n\\nTo minimise this error, we will use the Huber loss. \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "For our training update rule, we’ll use a fact that every Q function for some policy obeys the Bellman equation:\n",
    "\n",
    "  Qπ(s,a)=r+γQπ(s′,π(s′))\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error, δ\n",
    "\n",
    "  δ=Q(s,a)−(r+γmaxaQ(s′,a))\n",
    "\n",
    "To minimise this error, we will use the Huber loss. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building our Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting and processing rendered images from environment to componse image transformations\n",
    "# Once you run the cell it will display an example patch that it extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()])\n",
    "\n",
    "screen_width = 600 #based on the code from gym\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width //2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width //2,\n",
    "                           cart_location + view_width //2)\n",
    "    screen = screen[:, :, slice_range]\n",
    "    \n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none')\n",
    "plt.title('Example extrated screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (head): Linear(in_features=448, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_actions(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training......')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    \n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    plt.pause(0.001)\n",
    "    if is_python:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop - training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, \n",
    "                                           batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        action = select_actions(state)\n",
    "        _ , reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        \n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "            \n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
