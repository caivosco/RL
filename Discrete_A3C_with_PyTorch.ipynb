{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "w0 Ep: 1 | Ep_r: 15\n",
      "w0 Ep: 2 | Ep_r: 15\n",
      "w0 Ep: 3 | Ep_r: 15\n",
      "w0 Ep: 4 | Ep_r: 15\n",
      "w0 Ep: 5 | Ep_r: 15\n",
      "w0 Ep: 6 | Ep_r: 15\n",
      "w0 Ep: 7 | Ep_r: 15\n",
      "w0 Ep: 8 | Ep_r: 15\n",
      "w0 Ep: 9 | Ep_r: 15\n",
      "w0 Ep: 10 | Ep_r: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ref https://github.com/MorvanZhou/pytorch-A3C and using Pytorch=0.4.0\n",
    "# some theory on A3C https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2\n",
    "\n",
    "# loading dependencies\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import gym\n",
    "\n",
    "# first of all, we will define our optimizer (which will be shared in the multiprocessors)\n",
    "class SharedAdam(torch.optim.Adam):\n",
    "    \n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.9), eps=1e-8,weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                \n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()      \n",
    "\n",
    "# we define some functions\n",
    "def v_wrap(np_array, dtype=np.float32):\n",
    "    if np_array.dtype != dtype:\n",
    "        np_array = np_array.astype(dtype)\n",
    "    return torch.from_numpy(np_array)\n",
    "\n",
    "def set_init(layers):\n",
    "    for layer in layers:\n",
    "        nn.init.normal_(layer.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "def push_and_pull(opt, lnet, gnet, done, s_, bs, ba, br, gamma):\n",
    "    if done:\n",
    "        v_s_ = 0.\n",
    "    else:\n",
    "        v_s_ = lnet.forward(v_wrap(s_[None, :]))[-1].data.numpy()[0, 0]\n",
    "        \n",
    "    buffer_v_target = []\n",
    "    \n",
    "    for r in br[::-1]:\n",
    "        v_s_ = r + gamma * v_s_\n",
    "        buffer_v_target.append(v_s_)\n",
    "    buffer_v_target.reverse()\n",
    "    \n",
    "    loss = lnet.loss_func(v_wrap(np.vstack(bs)), \n",
    "                          v_wrap(np.array(ba), dtype=np.int64) if ba[0].dtype == np.int64 else v_wrap(np.vstack(ba)),\n",
    "                         v_wrap(np.array(buffer_v_target)[:, None]))\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    for lp, gp in zip(lnet.parameters(), gnet.parameters()):\n",
    "        gp._grad = lp.grad\n",
    "    opt.step()\n",
    "    \n",
    "    lnet.load_state_dict(gnet.state_dict())\n",
    "    \n",
    "def record(global_ep, global_ep_r, ep_r, res_queue, name):\n",
    "    with global_ep.get_lock():\n",
    "        global_ep.value += 1\n",
    "    with global_ep_r.get_lock():\n",
    "        if global_ep_r.value == 0.:\n",
    "            global_ep_r.value = ep_r\n",
    "        else:\n",
    "            global_ep_r.value = global_ep_r.value * 0.99 + ep_r * 0.01\n",
    "    res_queue.put(global_ep_r.value)\n",
    "    print(name, \"Ep:\", global_ep.value, \"| Ep_r: %.0f\" % global_ep_r.value,)\n",
    "\n",
    "\n",
    "#some useful definitions\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "UPDATE_GLOBAL_ITER = 10\n",
    "GAMMA = 0.9 #for the reward\n",
    "MAX_EP = 10 # number of epochs/episodes, you can change it\n",
    "\n",
    "#defining the environment rl\n",
    "env = gym.make('CartPole-v0')\n",
    "N_S = env.observation_space.shape[0] #states\n",
    "N_A = env.action_space.n  #actions\n",
    "\n",
    "# defining the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.pi1 = nn.Linear(s_dim, 100)\n",
    "        self.pi2 = nn.Linear(100, a_dim)\n",
    "        self.v1 = nn.Linear(s_dim, 100)\n",
    "        self.v2 = nn.Linear(100, 1)\n",
    "        set_init([self.pi1, self.pi2, self.v1, self.v2])\n",
    "        self.distribution = torch.distributions.Categorical\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pi1 = F.relu(self.pi1(x))\n",
    "        logits = self.pi2(pi1)\n",
    "        v1 = F.relu(self.v1(x))\n",
    "        values = self.v2(v1)\n",
    "        return logits, values\n",
    "    \n",
    "    def choose_action(self, s):\n",
    "        self.eval()\n",
    "        logits, _ = self.forward(s)\n",
    "        prob = F.softmax(logits, dim=1).data\n",
    "        m = self.distribution(prob)\n",
    "        return m.sample().numpy()[0]\n",
    "    \n",
    "    def loss_func(self, s, a, v_t):\n",
    "        self.train()\n",
    "        logits, values = self.forward(s)\n",
    "        td = v_t - values\n",
    "        c_loss = td.pow(2)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        m = self.distribution(probs)\n",
    "        exp_v = m.log_prob(a) * td.detach()\n",
    "        a_loss = -exp_v\n",
    "        total_loss = (c_loss + a_loss).mean()\n",
    "        return total_loss\n",
    "\n",
    "# defining the Process to run several in parallel\n",
    "class Worker(mp.Process):\n",
    "    def __init__(self, gnet, opt, global_ep, global_ep_r, res_queue, name):\n",
    "        super(Worker, self).__init__()\n",
    "        self.name = 'w%i' % name\n",
    "        self.g_ep, self.g_ep_r, self.res_queue = global_ep, global_ep_r, res_queue\n",
    "        self.gnet, self.opt = gnet, opt\n",
    "        self.lnet = Net(N_S, N_A) \n",
    "        self.env = gym.make('CartPole-v0').unwrapped\n",
    "        \n",
    "    def run(self):\n",
    "        total_step = 1\n",
    "        while self.g_ep.value < MAX_EP:\n",
    "            s = self.env.reset()\n",
    "            buffer_s, buffer_a, buffer_r = [], [], []\n",
    "            ep_r = 0.\n",
    "            while True:\n",
    "                if self.name == 'w0':\n",
    "                    self.env.render()\n",
    "                a = self.lnet.choose_action(v_wrap(s[None, :]))\n",
    "                s_, r, done, _ = self.env.step(a)\n",
    "                if done: r = -1\n",
    "                ep_r += r \n",
    "                buffer_a.append(a)\n",
    "                buffer_s.append(s)\n",
    "                buffer_r.append(r)\n",
    "                \n",
    "                if total_step % UPDATE_GLOBAL_ITER == 0 or done:\n",
    "                    push_and_pull(self.opt, self.lnet, self.gnet, done, s_, buffer_s, buffer_a, buffer_r, GAMMA)\n",
    "                    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "                    \n",
    "                    if done:\n",
    "                        record(self.g_ep, self.g_ep_r, ep_r, self.res_queue, self.name)\n",
    "                        break\n",
    "                s = s_\n",
    "                total_step += 1\n",
    "        self.res_queue.put(None)\n",
    "\n",
    "#now its time to run \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #running \n",
    "    gnet = Net(N_S, N_A)\n",
    "    gnet.share_memory()\n",
    "    opt = SharedAdam(gnet.parameters(), lr=0.0001)\n",
    "    global_ep, global_ep_r, res_queue = mp.Value('i', 0), mp.Value('d', 0.), mp.Queue()\n",
    "    \n",
    "    #parallel training \n",
    "    workers = [Worker(gnet, opt, global_ep, global_ep_r, res_queue, i) for i in range(mp.cpu_count())]\n",
    "    [w.start() for w in workers]\n",
    "    res = []\n",
    "    while True:\n",
    "        r = res_queue.get()\n",
    "        if r is not None:\n",
    "            res.append(r)\n",
    "        else:\n",
    "            break\n",
    "    [w.join() for w in workers]\n",
    "    \n",
    "    plt.plot(res)\n",
    "    plt.ylabel('Moving average ep reward')\n",
    "    plt.xlabel('Step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
